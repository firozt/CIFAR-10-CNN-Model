{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import Tensor\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting device (checking if gpu is available for faster training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset and applying data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10(batch_size):\n",
    "    # Transformations for the training data\n",
    "    train_trans = transforms.Compose([\n",
    "        transforms.RandomCrop(size=(32, 32), padding=4),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Transformations for the test data (only normalization)\n",
    "    test_trans = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Loading the datasets\n",
    "    train = torchvision.datasets.CIFAR10(root=\"../data\", train=True, transform=train_trans, download=True)\n",
    "    test = torchvision.datasets.CIFAR10(root=\"../data\", train=False, transform=test_trans, download=True)\n",
    "    \n",
    "    # Creating the DataLoaders\n",
    "    train_loader = DataLoader(train, batch_size, shuffle=True, pin_memory=False)\n",
    "    test_loader = DataLoader(test, batch_size, shuffle=False, pin_memory=False)\n",
    "    \n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_iter, test_iter = load_cifar10(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 Dataset Analysis\n",
    "Cifar-10 holds images if dimension 32x32\n",
    "Cifar images also contain colour using the rgb format\n",
    "Therefore each item in the dataset is of size 3x32x32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    This class is the main model, it is made out of 7 Backbone blocks\n",
    "    A backbone block is a class 'Block' defined below this one\n",
    "    The model uses a MLP classifier made out of 3 linear layers\n",
    "    All activation functions used are ReLU, and batch normalisation\n",
    "    is used after every block (done within the block node itself)\n",
    "    A block can also have a residual aspect to it, i.e O = f(x)+x,\n",
    "    a feature taken from the resnet model from the lectures\n",
    "    \n",
    "    -----------------------------------------------------------------------\n",
    "    \n",
    "    We can view the architecture as 3 groups of blocks, each serpated\n",
    "    by a pooling layer (either max or average) followed by a dropout operator\n",
    "    \n",
    "    The first group contains 2 blocks and an average pooling layer\n",
    "    The second group contains 2 blocks and a max pooling layer\n",
    "    The third group contains 3 blocks followed by a max pooling layer\n",
    "    \n",
    "    This then feeds into the spatial average pooling layer that passes\n",
    "    its value into the 3 layer MLP classifier which outputs a vector of\n",
    "    10 values\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_features):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # ==== hyper params ==== #\n",
    "        # block group outputs\n",
    "        B1_OUT = 32\n",
    "        B2_OUT = 64\n",
    "        B3_OUT = 128\n",
    "        self.END_OUT = B3_OUT\n",
    "        # mlp classifier layer outputs\n",
    "        L1_OUT = 1024\n",
    "        L2_OUT = 512\n",
    "        # ====================== #\n",
    "        \n",
    "        # pooling layers\n",
    "        self.maxpool = nn.MaxPool2d(2,2) \n",
    "        self.avgpool = nn.AvgPool2d(2,2) \n",
    "        \n",
    "        # blocks\n",
    "        self.block1 = Block(6,in_channels,B1_OUT, kernel_size=5, padding=2) \n",
    "        self.block2 = Block(6,B1_OUT,B1_OUT,kernel_size=5,padding=2)\n",
    "        self.block3 = Block(6,B1_OUT,B2_OUT, kernel_size=3, padding=1)\n",
    "        self.block4 = Block(6,B2_OUT,B2_OUT,kernel_size=3,padding=1,residual=True)\n",
    "        self.block5 = Block(6,B2_OUT,B3_OUT,kernel_size=3, padding=1)\n",
    "        self.block6 = Block(6,B3_OUT,B3_OUT,kernel_size=3,padding=1, residual=True)\n",
    "        self.block7 = Block(6,B3_OUT,B3_OUT,kernel_size=3,padding=1, residual=True)\n",
    "\n",
    "\n",
    "        # dropout layers\n",
    "        self.d1 = nn.Dropout(0.1)        \n",
    "        self.d2 = nn.Dropout(0.1)        \n",
    "        self.d3 = nn.Dropout(0.15)        \n",
    "        self.d4 = nn.Dropout(0.15)\n",
    "        self.d5 = nn.Dropout(0.15)\n",
    "        \n",
    "        # Linear Layers + spatial pooling\n",
    "        \n",
    "        self.spatial_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(self.END_OUT,L1_OUT)\n",
    "        self.linear2 = nn.Linear(L1_OUT,L2_OUT)\n",
    "        self.linear3 = nn.Linear(L2_OUT, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # block group 1\n",
    "        out = self.activation(self.block1(x))\n",
    "        out = self.activation(self.block2(out))\n",
    "        out = self.avgpool(out) #16x16\n",
    "        out = self.d1(out)\n",
    "        \n",
    "        # block group 2\n",
    "        out = self.activation(self.block3(out))\n",
    "        out = self.activation(self.block4(out))\n",
    "        out = self.maxpool(out) # 8x8\n",
    "        out = self.d2(out)\n",
    "\n",
    "        # block group 3\n",
    "        out = self.activation(self.block5(out))\n",
    "        out = self.activation(self.block6(out))\n",
    "        out = self.activation(self.block7(out))\n",
    "        out = self.maxpool(out) # 4x4\n",
    "        out = self.d3(out)\n",
    "\n",
    "        # linear MLP layers and spatial pooling\n",
    "        \n",
    "        out = self.spatial_pool(out)\n",
    "        out = out.reshape(-1 , self.END_OUT)\n",
    "\n",
    "        out = self.linear1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.d4(out)\n",
    "\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.d5(out)\n",
    "\n",
    "        out = self.linear3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, k, in_channels, out_channels, padding=2, kernel_size=5, stride=1,residual=False , batch_norm=True):\n",
    "        \"\"\"\n",
    "        This class defines the block backbone mentioned on the coursework slides\n",
    "        The adjustments ive made to the architecture is that the vector a is\n",
    "        calculated from not only 1 linear layer of weights, but now a 3 layer MLP\n",
    "        the output is still a vector of k weights.\n",
    "        Another adjustment ive made is that after every block it applies batch normalisation\n",
    "        to its output and returns it.\n",
    "        \n",
    "        The batch norm is calculated from the O value, not each individual (k) convolution\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        super(Block, self).__init__()\n",
    "        self.batch_norm = batch_norm # if true will apply batch norm after this block\n",
    "        self.residual = residual # if true will add the input to the output (residual connection)\n",
    "        # === hyper params === #\n",
    "        # output for mlp layer that calculates\n",
    "        L1_OUT = 512\n",
    "        L2_OUT = 512\n",
    "        # ==================== #\n",
    "        \n",
    "        self.k = k\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "        # defining weight calculations (a)\n",
    "        self.spatialAvgPool = nn.AdaptiveAvgPool2d(1)\n",
    "        # flatten\n",
    "        self.aLinear1 = nn.Linear(in_features=in_channels, out_features=L1_OUT)\n",
    "        self.aLinear2 = nn.Linear(in_features=L1_OUT,out_features=L2_OUT)\n",
    "        self.aLinear3 = nn.Linear(L2_OUT,k)\n",
    "        self.activation = nn.ReLU()\n",
    "        # defining the convolution section\n",
    "        # creates k convolutions\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size ,padding=padding, stride=stride) for _ in range(k)])\n",
    "        # self.batch_norms = nn.ModuleList([nn.BatchNorm2d(out_channels) for _ in range(k)])\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "        # apply spatial average pooling\n",
    "        pooled: Tensor = self.spatialAvgPool(x)\n",
    "        # flatten \n",
    "        pooled = pooled.view(pooled.size(0), -1) \n",
    "        # calculate a\n",
    "        a: Tensor = self.activation(self.aLinear1(pooled))\n",
    "        a = self.activation(self.aLinear2(a))\n",
    "        a = self.aLinear3(a)\n",
    "\n",
    "        O: Tensor = torch.zeros_like(self.convs[0](x))\n",
    "\n",
    "        # Apply the convolutions and accumulate the weighted sum\n",
    "        for i in range(self.k):\n",
    "            conv = self.convs[i](x) # conv_i\n",
    "            # the view is used to reshape the 1d tensor (a[:, i]) into a 4d tensor that is addition compatible with conv(x)\n",
    "            O = O + a[:, i].view(-1, 1, 1, 1) * conv # this calculates a_1 conv_1 + ... + a_k conv_k, also applies activation function to it\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            O = self.batch_norm(O)\n",
    "        \n",
    "        if self.residual:\n",
    "            O= O + x\n",
    "        \n",
    "        return O # returns [batchsize, c, h, w]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumulator class from the utils file, used to calculate metrics\n",
    "\n",
    "class Accumulator:\n",
    "    \n",
    "    def __init__(self, n) -> None:\n",
    "        self.data = [0.0] * n\n",
    "    \n",
    "    # deletes all data and information stored\n",
    "    def reset(self) -> None:\n",
    "        self.data = [0.0] * len(self.data)\n",
    "    \n",
    "    # takes in n inputs, for each arg it adds it to its corresponding index in data\n",
    "    def add(self, *args) -> None:\n",
    "        self.data = [a + float(b) for a,b in zip(self.data, args)]\n",
    "    \n",
    "    # allows the indexing operator to be used\n",
    "    def __getitem__(self, idx) -> any:\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def toList(self) -> list:\n",
    "        return self.data\n",
    "    \n",
    "    def percentage(self, index, total):\n",
    "        return 100 * self[index] / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "Training loop for my model.\n",
    "It trains over all the items in the trainset\n",
    "For each iteration it calculates the loss and uses that to calculate\n",
    "the gradient of the loss wrt the weight and takes a step in that direction to\n",
    "minimise the loss (which then maximises accuracy). Once it iterates through\n",
    "all the values, it then stores the average loss and training accuracy for\n",
    "graph plotting\n",
    "\n",
    "The testing stage is done right after training and the model is explicitly \n",
    "set to evaluation mode so it doesnt learn from the test set.\n",
    "It then uses the test set to calculate the respective test accuracy for this epoch\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def trainingLoop(num_epoch, train_iter, test_iter, net, loss_function, optimizer) -> tuple[int, list[float], list[float], list[float]]:\n",
    "    loss_values_train = [] \n",
    "    loss_values_test = []\n",
    "    training_accuracy_values = []  \n",
    "    testing_accuracy_values = []\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        # train on trianing set to obtain updated weights\n",
    "        train_metrics = train(train_iter, net, loss_function, optimizer) # tuple (loss , accuracy)\n",
    "        # testing new weights on unseen data\n",
    "        test_metrics = test(test_iter, net, loss_function) # tuple (loss, accuracy)\n",
    "        # data for plotting\n",
    "        loss_values_train.append(train_metrics[0])\n",
    "        loss_values_test.append(test_metrics[0])\n",
    "        training_accuracy_values.append(train_metrics[1]) \n",
    "        testing_accuracy_values.append(test_metrics[1])\n",
    "\n",
    "        # print(f'Epoch {epoch+1}, Average Loss (train) : {loss_values_train[-1]}, Average Loss (test){loss_values_test[-1]}, Training Accuracy: {training_accuracy_values[-1]}, Testing Accuracy {testing_accuracy_values[-1]}')\n",
    "        print(f'Epoch {epoch+1}, Average Loss (train) : {loss_values_train[-1]}, Training Accuracy: {training_accuracy_values[-1]}, Testing Accuracy {testing_accuracy_values[-1]}')\n",
    "        \n",
    "    return(num_epoch, loss_values_train, loss_values_test, training_accuracy_values, testing_accuracy_values)\n",
    "\n",
    "\n",
    "def test(data_iter, net, loss_function) -> float:\n",
    "    net.eval() # set to testing mode\n",
    "    \n",
    "    metrics = Accumulator(2) # [correct, total]\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Get model predictions\n",
    "            \n",
    "            y_hat = net(X)\n",
    "            loss = loss_function(y_hat, y)\n",
    "            total_loss += loss.item()\n",
    "            # Convert output to predicted class labels\n",
    "            _, predicted_labels = torch.max(y_hat, 1)\n",
    "            # Accumulate the number of correct predictions and the total to metrics\n",
    "            metrics.add((predicted_labels == y).sum().item(), y.size(0))\n",
    "            \n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = metrics[0] / metrics[1]\n",
    "    avg_loss = total_loss / metrics[1]\n",
    "    # Set the network back to training mode\n",
    "    net.train()\n",
    "    \n",
    "    return (avg_loss,accuracy)\n",
    "\n",
    "def train(data_iter, net, loss_function, optimizer) -> tuple[float , float]:\n",
    "    net.train()\n",
    "    \n",
    "    metrics = Accumulator(3)  # [sum of losses, correct predictions, total predictions]\n",
    "    for X, Y in data_iter:  # get x and corresponding y value\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        \n",
    "        y_hat = net(X)  # get prediction\n",
    "        loss = loss_function(y_hat, Y)  # loss of this specific value\n",
    "        optimizer.zero_grad()  # clear gradient\n",
    "        loss.backward()  # calculate derivative of loss w.r.t the weight\n",
    "        optimizer.step()  # change weight values accordingly\n",
    "        # add the sum of all correct predictions and total which is size of Y \n",
    "        metrics.add(loss*X.shape[0], (torch.max(y_hat, 1)[1] == Y).float().sum().item(), X.size(0))\n",
    "    # Calculate average loss and accuracy from the accumulated values\n",
    "    avg_loss = metrics[0] / metrics[2]\n",
    "    accuracy = metrics[1] / metrics[2]\n",
    "    \n",
    "    return (avg_loss, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to plot test and training accuracy aswell as training loss\n",
    "\"\"\"\n",
    "def plotTraining(num_epoch, loss_values_train, loss_values_test, train_accuracy_values, test_accuracy_values):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "    # Training Loss\n",
    "    color_train = 'tab:blue'\n",
    "    color_test = 'tab:orange'\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.plot(range(1, num_epoch+1), loss_values_train, '-o', color=color_train, label='Train Loss')\n",
    "    # ax1.plot(range(1, num_epoch+1), loss_values_test, '-o', color=color_test, label='Test Loss')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.set_title('Loss')\n",
    "\n",
    "    # Training Accuracy and Testing Accuracy\n",
    "    color_train = 'tab:blue'\n",
    "    color_test = 'tab:orange'\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.plot(range(1, num_epoch+1), train_accuracy_values, '-s', color=color_train, label='Train Accuracy')\n",
    "    ax2.plot(range(1, num_epoch+1), test_accuracy_values, '-^', color=color_test, label='Test Accuracy')\n",
    "    ax2.legend(loc='upper left')\n",
    "    ax2.set_title('Accuracy')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple inistialsation of weights using xavier normal or linear\n",
    "and convolutional layers\n",
    "\"\"\"\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_normal_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(3, 10).to(device)\n",
    "# model = Model(3, 10)\n",
    "model.apply(init_weights)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "lr = 0.002\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 65\n",
    "params = trainingLoop(num_epochs,train_iter,test_iter ,model, loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTraining(*params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
