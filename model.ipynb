{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import Tensor\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting device (checking if gpu is available for faster training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset and applying data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10(batch_size):\n",
    "    # Transformations for the training data\n",
    "    train_trans = transforms.Compose([\n",
    "        transforms.RandomCrop(size=(32, 32), padding=4),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Transformations for the test data (minimal, without augmentation)\n",
    "    test_trans = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Loading the datasets\n",
    "    train = torchvision.datasets.CIFAR10(root=\"../data\", train=True, transform=train_trans, download=True)\n",
    "    test = torchvision.datasets.CIFAR10(root=\"../data\", train=False, transform=test_trans, download=True)\n",
    "    \n",
    "    # Creating the DataLoaders\n",
    "    train_loader = DataLoader(train, batch_size, shuffle=True, pin_memory=False)\n",
    "    test_loader = DataLoader(test, batch_size, shuffle=False, pin_memory=False)\n",
    "    \n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_iter, test_iter = load_cifar10(BATCH_SIZE)\n",
    "for data in train_iter:\n",
    "    print(data[0].size())\n",
    "    print(data[1].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 Dataset Analysis\n",
    "Cifar-10 holds images if dimension 32x32\n",
    "Cifar images also contain colour using the rgb format\n",
    "Therefore each item in the dataset is of size 3x32x32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    This class is the main model, it is made out of 7 Backbone blocks\n",
    "    A backbone block is a class 'Block' defined below this one\n",
    "    The model uses a MLP classifier made out of 3 linear layers\n",
    "    All activation functions used are ReLU, and batch normalisation\n",
    "    is used after every block (done within the block node itself)\n",
    "    A block can also have a residual aspect to it, i.e O = f(x)+x,\n",
    "    a feature taken from the resnet model from the lectures\n",
    "    \n",
    "    -----------------------------------------------------------------------\n",
    "    \n",
    "    We can view the architecture as 3 groups of blocks, each serpated\n",
    "    by a pooling layer (either max or average) followed by a dropout operator\n",
    "    \n",
    "    The first group contains 2 blocks and an average pooling layer\n",
    "    The second group contains 2 blocks and a max pooling layer\n",
    "    The third group contains 3 blocks followed by a max pooling layer\n",
    "    \n",
    "    This then feeds into the spatial average pooling layer that passes\n",
    "    its value into the 3 layer MLP classifier which outputs a vector of\n",
    "    10 values\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_features):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # ==== hyper params ==== #\n",
    "        # block group outputs\n",
    "        B1_OUT = 32\n",
    "        B2_OUT = 64\n",
    "        B3_OUT = 128\n",
    "        self.END_OUT = B3_OUT\n",
    "        # mlp classifier layer outputs\n",
    "        L1_OUT = 1024\n",
    "        L2_OUT = 512\n",
    "        # ====================== #\n",
    "        \n",
    "        # pooling layers\n",
    "        self.maxpool = nn.MaxPool2d(2,2) \n",
    "        self.avgpool = nn.AvgPool2d(2,2) \n",
    "        \n",
    "        # blocks\n",
    "        self.block1 = Block(6,in_channels,B1_OUT, kernel_size=5, padding=2) \n",
    "        self.block2 = Block(6,B1_OUT,B1_OUT,kernel_size=5,padding=2)\n",
    "        self.block3 = Block(6,B1_OUT,B2_OUT, kernel_size=3, padding=1)\n",
    "        self.block4 = Block(6,B2_OUT,B2_OUT,kernel_size=3,padding=1,residual=True)\n",
    "        self.block5 = Block(6,B2_OUT,B3_OUT,kernel_size=3, padding=1)\n",
    "        self.block6 = Block(6,B3_OUT,B3_OUT,kernel_size=3,padding=1, residual=True)\n",
    "        self.block7 = Block(6,B3_OUT,B3_OUT,kernel_size=3,padding=1, residual=True)\n",
    "\n",
    "\n",
    "        # dropout layers\n",
    "        self.d1 = nn.Dropout(0.1)        \n",
    "        self.d2 = nn.Dropout(0.1)        \n",
    "        self.d3 = nn.Dropout(0.15)        \n",
    "        self.d4 = nn.Dropout(0.15)\n",
    "        self.d5 = nn.Dropout(0.15)\n",
    "        \n",
    "        # Linear Layers + spatial pooling\n",
    "        \n",
    "        self.spatial_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(self.END_OUT,L1_OUT)\n",
    "        self.linear2 = nn.Linear(L1_OUT,L2_OUT)\n",
    "        self.linear3 = nn.Linear(L2_OUT, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # block group 1\n",
    "        out = self.activation(self.block1(x))\n",
    "        out = self.activation(self.block2(out))\n",
    "        out = self.avgpool(out) #16x16\n",
    "        out = self.d1(out)\n",
    "        \n",
    "        # block group 2\n",
    "        out = self.activation(self.block3(out))\n",
    "        out = self.activation(self.block4(out))\n",
    "        out = self.maxpool(out) # 8x8\n",
    "        out = self.d2(out)\n",
    "\n",
    "        # block group 3\n",
    "        out = self.activation(self.block5(out))\n",
    "        out = self.activation(self.block6(out))\n",
    "        out = self.activation(self.block7(out))\n",
    "        out = self.maxpool(out) # 4x4\n",
    "        out = self.d3(out)\n",
    "\n",
    "        # linear MLP layers and spatial pooling\n",
    "        \n",
    "        out = self.spatial_pool(out)\n",
    "        out = out.reshape(-1 , self.END_OUT)\n",
    "\n",
    "        out = self.linear1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.d4(out)\n",
    "\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.d5(out)\n",
    "\n",
    "        out = self.linear3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, k, in_channels, out_channels, padding=2, kernel_size=5, stride=1,residual=False , batch_norm=True):\n",
    "        \"\"\"\n",
    "        This class defines the block backbone mentioned on the coursework slides\n",
    "        The adjustments ive made to the architecture is that the vector a is\n",
    "        calculated from not only 1 linear layer of weights, but now a 3 layer MLP\n",
    "        the output is still a vector of k weights.\n",
    "        Another adjustment ive made is that after every block it applies batch normalisation\n",
    "        to its output and returns it.\n",
    "        \n",
    "        The batch norm is calculated from the O value, not each individual (k) convolution\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        super(Block, self).__init__()\n",
    "        self.batch_norm = batch_norm # if true will apply batch norm after this block\n",
    "        self.residual = residual # if true will add the input to the output (residual connection)\n",
    "        # === hyper params === #\n",
    "        # output for mlp layer that calculates\n",
    "        L1_OUT = 512\n",
    "        L2_OUT = 512\n",
    "        # ==================== #\n",
    "        \n",
    "        self.k = k\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "        # defining weight calculations (a)\n",
    "        self.spatialAvgPool = nn.AdaptiveAvgPool2d(1)\n",
    "        # flatten\n",
    "        self.aLinear1 = nn.Linear(in_features=in_channels, out_features=L1_OUT)\n",
    "        self.aLinear2 = nn.Linear(in_features=L1_OUT,out_features=L2_OUT)\n",
    "        self.aLinear3 = nn.Linear(L2_OUT,k)\n",
    "        self.activation = nn.ReLU()\n",
    "        # defining the convolution section\n",
    "        # creates k convolutions\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size ,padding=padding, stride=stride) for _ in range(k)])\n",
    "        # self.batch_norms = nn.ModuleList([nn.BatchNorm2d(out_channels) for _ in range(k)])\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "        # apply spatial average pooling\n",
    "        pooled: Tensor = self.spatialAvgPool(x)\n",
    "        # flatten \n",
    "        pooled = pooled.view(pooled.size(0), -1) \n",
    "        # calculate a\n",
    "        a: Tensor = self.activation(self.aLinear1(pooled))\n",
    "        a = self.activation(self.aLinear2(a))\n",
    "        a = self.aLinear3(a)\n",
    "\n",
    "        O: Tensor = torch.zeros_like(self.convs[0](x))\n",
    "\n",
    "        # Apply the convolutions and accumulate the weighted sum\n",
    "        for i in range(self.k):\n",
    "            conv = self.convs[i](x) # conv_i\n",
    "            # the view is used to reshape the 1d tensor (a[:, i]) into a 4d tensor that is addition compatible with conv(x)\n",
    "            O = O + a[:, i].view(-1, 1, 1, 1) * conv # this calculates a_1 conv_1 + ... + a_k conv_k, also applies activation function to it\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            O = self.batch_norm(O)\n",
    "        \n",
    "        if self.residual:\n",
    "            O= O + x\n",
    "        \n",
    "        return O # returns [batchsize, c, h, w]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumulator class from the utils file, used to calculate metrics\n",
    "\n",
    "class Accumulator:\n",
    "    \n",
    "    def __init__(self, n) -> None:\n",
    "        self.data = [0.0] * n\n",
    "    \n",
    "    # deletes all data and information stored\n",
    "    def reset(self) -> None:\n",
    "        self.data = [0.0] * len(self.data)\n",
    "    \n",
    "    # takes in n inputs, for each arg it adds it to its corresponding index in data\n",
    "    def add(self, *args) -> None:\n",
    "        self.data = [a + float(b) for a,b in zip(self.data, args)]\n",
    "    \n",
    "    # allows the indexing operator to be used\n",
    "    def __getitem__(self, idx) -> any:\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def toList(self) -> list:\n",
    "        return self.data\n",
    "    \n",
    "    def percentage(self, index, total):\n",
    "        return 100 * self[index] / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "Training loop for my model.\n",
    "It trains over all the items in the trainset\n",
    "For each iteration it calculates the loss and uses that to calculate\n",
    "the gradient of the loss wrt the weight and takes a step in that direction to\n",
    "minimise the loss (which then maximises accuracy). Once it iterates through\n",
    "all the values, it then stores the average loss and training accuracy for\n",
    "graph plotting\n",
    "\n",
    "The testing stage is done right after training and the model is explicitly \n",
    "set to evaluation mode so it doesnt learn from the test set.\n",
    "It then uses the test set to calculate the respective test accuracy for this epoch\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def trainingLoop(num_epoch, train_iter, test_iter, net, loss_function, optimizer) -> tuple[int, list[float], list[float], list[float]]:\n",
    "    loss_values = [] \n",
    "    training_accuracy_values = []  \n",
    "    testing_accuracy_values = []\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        # train on trianing set to obtain updated weights\n",
    "        train_metrics = train(train_iter, net, loss_function, optimizer) # tuple (loss , accuracy)\n",
    "        # testing new weights on unseen data\n",
    "        test_accuracy = test(test_iter, net)\n",
    "        # data for plotting\n",
    "        loss_values.append(train_metrics[0])\n",
    "        training_accuracy_values.append(train_metrics[1]) \n",
    "        testing_accuracy_values.append(test_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {loss_values[-1]}, Training Accuracy: {training_accuracy_values[-1]}, Testing Accuracy {testing_accuracy_values[-1]}')\n",
    "        \n",
    "    return(num_epoch, loss_values, training_accuracy_values, testing_accuracy_values)\n",
    "\n",
    "\n",
    "def test(data_iter, net) -> float:\n",
    "    net.eval() # set to testing mode\n",
    "    \n",
    "    metrics = Accumulator(2) # [correct, total]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Get model predictions\n",
    "            y_hat = net(X)\n",
    "            # Convert probabilities to predicted class labels\n",
    "            _, predicted_labels = torch.max(y_hat, 1)\n",
    "            # Accumulate the number of correct predictions and the total to metrics\n",
    "            metrics.add((predicted_labels == y).sum().item(), y.size(0))\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = metrics[0] / metrics[1]\n",
    "    \n",
    "    # Set the network back to training mode\n",
    "    net.train()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def train(data_iter, net, loss_function, optimizer) -> tuple[float , float]:\n",
    "    net.train()\n",
    "    \n",
    "    metrics = Accumulator(3)  # [sum of losses, correct predictions, total predictions]\n",
    "    for X, Y in data_iter:  # get x and corresponding y value\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        \n",
    "        y_hat = net(X)  # get prediction\n",
    "        loss = loss_function(y_hat, Y)  # loss of this specific value\n",
    "        optimizer.zero_grad()  # clear gradient\n",
    "        loss.backward()  # calculate derivative of loss w.r.t the weight\n",
    "        optimizer.step()  # change weight values accordingly\n",
    "        # add the sum of all equivilant, total is size of Y \n",
    "        metrics.add(loss.item(), (torch.max(y_hat, 1)[1] == Y).float().sum().item(), Y.size(0))\n",
    "    # Calculate average loss and accuracy from the accumulated values\n",
    "    avg_loss = metrics[0] / metrics[2]\n",
    "    accuracy = metrics[1] / metrics[2]\n",
    "    \n",
    "    return (avg_loss, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to plot test and training accuracy aswell as training loss\n",
    "\"\"\"\n",
    "def plotTraining(num_epoch, loss_values, train_accuracy_values, test_accuracy_values):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Training Loss\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Train Loss', color=color)\n",
    "    ax1.plot(range(1, num_epoch+1), loss_values, '-o', color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Create a second y-axis to share the same x-axis\n",
    "    ax2 = ax1.twinx()  \n",
    "\n",
    "    # Training Accuracy and Testing Accuracy\n",
    "    color_train = 'tab:blue'\n",
    "    color_test = 'tab:orange'\n",
    "    ax2.set_ylabel('Accuracy', color=color_train)  # We already handled the x-label with ax1\n",
    "    # Plot training accuracy on ax2\n",
    "    ax2.plot(range(1, num_epoch+1), train_accuracy_values, '-s', color=color_train, label='Train Accuracy')\n",
    "    # Plot testing accuracy on ax2 as well\n",
    "    ax2.plot(range(1, num_epoch+1), test_accuracy_values, '-^', color=color_test, label='Test Accuracy')\n",
    "    ax2.tick_params(axis='y', labelcolor=color_train)\n",
    "    ax2.legend(loc='upper left')\n",
    "\n",
    "    fig.tight_layout()  # Otherwise the right y-label is slightly clipped\n",
    "    plt.title('Training Loss and Accuracy')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple inistialsation of weights using xavier normal or linear\n",
    "and convolutional layers\n",
    "\"\"\"\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        # torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(3, 10).to(device)\n",
    "# model = Model(3, 10)\n",
    "model.apply(init_weights)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "lr = 0.002\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.030700880861282347, Training Accuracy: 0.24652, Testing Accuracy 0.3161\n",
      "Epoch 2, Average Loss: 0.026120075430870054, Training Accuracy: 0.35732, Testing Accuracy 0.422\n",
      "Epoch 3, Average Loss: 0.023002301712036134, Training Accuracy: 0.45316, Testing Accuracy 0.4966\n",
      "Epoch 4, Average Loss: 0.02047726799607277, Training Accuracy: 0.52686, Testing Accuracy 0.5448\n",
      "Epoch 5, Average Loss: 0.01878171049118042, Training Accuracy: 0.56968, Testing Accuracy 0.6008\n",
      "Epoch 6, Average Loss: 0.017334357171058655, Training Accuracy: 0.6085, Testing Accuracy 0.6368\n",
      "Epoch 7, Average Loss: 0.016287829840183256, Training Accuracy: 0.63242, Testing Accuracy 0.667\n",
      "Epoch 8, Average Loss: 0.015540613657236099, Training Accuracy: 0.65214, Testing Accuracy 0.6723\n",
      "Epoch 9, Average Loss: 0.014765454753637313, Training Accuracy: 0.67028, Testing Accuracy 0.7026\n",
      "Epoch 10, Average Loss: 0.014266061156392097, Training Accuracy: 0.68338, Testing Accuracy 0.683\n",
      "Epoch 11, Average Loss: 0.013563726760149003, Training Accuracy: 0.69896, Testing Accuracy 0.7196\n",
      "Epoch 12, Average Loss: 0.01320673007786274, Training Accuracy: 0.7069, Testing Accuracy 0.7572\n",
      "Epoch 13, Average Loss: 0.012728627400994301, Training Accuracy: 0.71798, Testing Accuracy 0.7537\n",
      "Epoch 14, Average Loss: 0.012256000033020973, Training Accuracy: 0.73042, Testing Accuracy 0.7687\n",
      "Epoch 15, Average Loss: 0.011809577459692955, Training Accuracy: 0.74104, Testing Accuracy 0.7664\n",
      "Epoch 16, Average Loss: 0.01141394900918007, Training Accuracy: 0.74704, Testing Accuracy 0.7829\n",
      "Epoch 17, Average Loss: 0.01163246071755886, Training Accuracy: 0.74442, Testing Accuracy 0.7918\n",
      "Epoch 18, Average Loss: 0.010741254024505615, Training Accuracy: 0.7645, Testing Accuracy 0.7896\n",
      "Epoch 19, Average Loss: 0.010403583897948265, Training Accuracy: 0.77152, Testing Accuracy 0.7992\n",
      "Epoch 20, Average Loss: 0.01005498587101698, Training Accuracy: 0.78142, Testing Accuracy 0.8112\n",
      "Epoch 21, Average Loss: 0.00983494060754776, Training Accuracy: 0.78644, Testing Accuracy 0.7993\n",
      "Epoch 22, Average Loss: 0.009589950184226035, Training Accuracy: 0.78974, Testing Accuracy 0.8247\n",
      "Epoch 23, Average Loss: 0.00949178970694542, Training Accuracy: 0.7927, Testing Accuracy 0.8151\n",
      "Epoch 24, Average Loss: 0.009165694822669029, Training Accuracy: 0.79898, Testing Accuracy 0.8235\n",
      "Epoch 25, Average Loss: 0.009044284714758396, Training Accuracy: 0.80252, Testing Accuracy 0.8201\n",
      "Epoch 26, Average Loss: 0.008845763543844223, Training Accuracy: 0.80762, Testing Accuracy 0.826\n",
      "Epoch 27, Average Loss: 0.008676117440909147, Training Accuracy: 0.81116, Testing Accuracy 0.8255\n",
      "Epoch 28, Average Loss: 0.008594086292386055, Training Accuracy: 0.81142, Testing Accuracy 0.8271\n",
      "Epoch 29, Average Loss: 0.008392009464204311, Training Accuracy: 0.81594, Testing Accuracy 0.8356\n",
      "Epoch 30, Average Loss: 0.008297637007534504, Training Accuracy: 0.81778, Testing Accuracy 0.8315\n",
      "Epoch 31, Average Loss: 0.008182707984745503, Training Accuracy: 0.82262, Testing Accuracy 0.832\n",
      "Epoch 32, Average Loss: 0.00810706354022026, Training Accuracy: 0.8227, Testing Accuracy 0.8387\n",
      "Epoch 33, Average Loss: 0.008051489864885807, Training Accuracy: 0.82258, Testing Accuracy 0.8299\n",
      "Epoch 34, Average Loss: 0.007867357162833214, Training Accuracy: 0.8271, Testing Accuracy 0.8406\n",
      "Epoch 35, Average Loss: 0.0077926491793990135, Training Accuracy: 0.82884, Testing Accuracy 0.8504\n",
      "Epoch 36, Average Loss: 0.007814747556746006, Training Accuracy: 0.82958, Testing Accuracy 0.85\n",
      "Epoch 37, Average Loss: 0.0076888601803779605, Training Accuracy: 0.8325, Testing Accuracy 0.8478\n",
      "Epoch 38, Average Loss: 0.0075364359942078595, Training Accuracy: 0.83472, Testing Accuracy 0.8487\n",
      "Epoch 39, Average Loss: 0.007499930479824543, Training Accuracy: 0.83486, Testing Accuracy 0.8329\n",
      "Epoch 40, Average Loss: 0.007459043165743351, Training Accuracy: 0.83804, Testing Accuracy 0.8537\n",
      "Epoch 41, Average Loss: 0.007369172372817993, Training Accuracy: 0.83846, Testing Accuracy 0.8461\n",
      "Epoch 42, Average Loss: 0.007372003373205662, Training Accuracy: 0.83892, Testing Accuracy 0.8581\n",
      "Epoch 43, Average Loss: 0.007211848224699498, Training Accuracy: 0.84244, Testing Accuracy 0.8465\n",
      "Epoch 44, Average Loss: 0.007149473993778229, Training Accuracy: 0.8432, Testing Accuracy 0.8516\n",
      "Epoch 45, Average Loss: 0.007129616550803184, Training Accuracy: 0.84414, Testing Accuracy 0.8593\n",
      "Epoch 46, Average Loss: 0.007035373618006706, Training Accuracy: 0.84598, Testing Accuracy 0.8493\n",
      "Epoch 47, Average Loss: 0.007081595090329647, Training Accuracy: 0.8452, Testing Accuracy 0.852\n",
      "Epoch 48, Average Loss: 0.006922430862784386, Training Accuracy: 0.84958, Testing Accuracy 0.8597\n",
      "Epoch 49, Average Loss: 0.006972085899710655, Training Accuracy: 0.84832, Testing Accuracy 0.86\n",
      "Epoch 50, Average Loss: 0.006874375079572201, Training Accuracy: 0.85092, Testing Accuracy 0.8548\n",
      "Epoch 51, Average Loss: 0.006858095426261425, Training Accuracy: 0.85034, Testing Accuracy 0.8599\n",
      "Epoch 52, Average Loss: 0.006749263800829649, Training Accuracy: 0.85232, Testing Accuracy 0.8532\n",
      "Epoch 53, Average Loss: 0.006707447543144226, Training Accuracy: 0.85332, Testing Accuracy 0.8511\n",
      "Epoch 54, Average Loss: 0.006680034545063973, Training Accuracy: 0.85286, Testing Accuracy 0.8629\n",
      "Epoch 55, Average Loss: 0.006587572067081928, Training Accuracy: 0.85466, Testing Accuracy 0.8622\n",
      "Epoch 56, Average Loss: 0.006586044778525829, Training Accuracy: 0.85544, Testing Accuracy 0.8644\n",
      "Epoch 57, Average Loss: 0.006518292239904404, Training Accuracy: 0.85726, Testing Accuracy 0.8637\n",
      "Epoch 58, Average Loss: 0.006523887241184712, Training Accuracy: 0.85804, Testing Accuracy 0.867\n",
      "Epoch 59, Average Loss: 0.006558225817680359, Training Accuracy: 0.85722, Testing Accuracy 0.8635\n",
      "Epoch 60, Average Loss: 0.006465278924405575, Training Accuracy: 0.85812, Testing Accuracy 0.8635\n",
      "Epoch 61, Average Loss: 0.006362661198079586, Training Accuracy: 0.86048, Testing Accuracy 0.8575\n",
      "Epoch 62, Average Loss: 0.006371103367805481, Training Accuracy: 0.86048, Testing Accuracy 0.8596\n",
      "Epoch 63, Average Loss: 0.006398114694356918, Training Accuracy: 0.86012, Testing Accuracy 0.8646\n",
      "Epoch 64, Average Loss: 0.006376579568982125, Training Accuracy: 0.86028, Testing Accuracy 0.8655\n",
      "Epoch 65, Average Loss: 0.006313851077258587, Training Accuracy: 0.86218, Testing Accuracy 0.8695\n",
      "Epoch 66, Average Loss: 0.006269663101732731, Training Accuracy: 0.86186, Testing Accuracy 0.8731\n",
      "Epoch 67, Average Loss: 0.006217775111496448, Training Accuracy: 0.86322, Testing Accuracy 0.8748\n",
      "Epoch 68, Average Loss: 0.006221451899409294, Training Accuracy: 0.86542, Testing Accuracy 0.8728\n",
      "Epoch 69, Average Loss: 0.006191637432873249, Training Accuracy: 0.8644, Testing Accuracy 0.8735\n",
      "Epoch 70, Average Loss: 0.006198090265095234, Training Accuracy: 0.86498, Testing Accuracy 0.8673\n",
      "Epoch 71, Average Loss: 0.006062255953848362, Training Accuracy: 0.86772, Testing Accuracy 0.8644\n",
      "Epoch 72, Average Loss: 0.006099075189232826, Training Accuracy: 0.86684, Testing Accuracy 0.8782\n",
      "Epoch 73, Average Loss: 0.006114737604856491, Training Accuracy: 0.86618, Testing Accuracy 0.8729\n",
      "Epoch 74, Average Loss: 0.005974969516098499, Training Accuracy: 0.87018, Testing Accuracy 0.8734\n",
      "Epoch 75, Average Loss: 0.006009716085791588, Training Accuracy: 0.866, Testing Accuracy 0.8753\n",
      "Epoch 76, Average Loss: 0.005989575522243977, Training Accuracy: 0.86888, Testing Accuracy 0.8685\n",
      "Epoch 77, Average Loss: 0.006010174442529679, Training Accuracy: 0.8669, Testing Accuracy 0.8792\n",
      "Epoch 78, Average Loss: 0.006042592054307461, Training Accuracy: 0.86766, Testing Accuracy 0.8786\n",
      "Epoch 79, Average Loss: 0.006039025081694126, Training Accuracy: 0.86806, Testing Accuracy 0.8704\n",
      "Epoch 80, Average Loss: 0.005992233383208513, Training Accuracy: 0.86942, Testing Accuracy 0.8706\n",
      "Epoch 81, Average Loss: 0.005951685771048069, Training Accuracy: 0.86958, Testing Accuracy 0.8616\n",
      "Epoch 82, Average Loss: 0.005878923165798187, Training Accuracy: 0.8698, Testing Accuracy 0.8764\n",
      "Epoch 83, Average Loss: 0.005894467604458332, Training Accuracy: 0.87142, Testing Accuracy 0.8794\n",
      "Epoch 84, Average Loss: 0.005757501686811447, Training Accuracy: 0.87342, Testing Accuracy 0.8799\n",
      "Epoch 85, Average Loss: 0.005876575773954391, Training Accuracy: 0.8728, Testing Accuracy 0.8769\n",
      "Epoch 86, Average Loss: 0.0058402170068025585, Training Accuracy: 0.87162, Testing Accuracy 0.8748\n",
      "Epoch 87, Average Loss: 0.005825800138711929, Training Accuracy: 0.87326, Testing Accuracy 0.8783\n",
      "Epoch 88, Average Loss: 0.005766935670673847, Training Accuracy: 0.87366, Testing Accuracy 0.8715\n",
      "Epoch 89, Average Loss: 0.005699509461969138, Training Accuracy: 0.87538, Testing Accuracy 0.8708\n",
      "Epoch 90, Average Loss: 0.0057159285727143285, Training Accuracy: 0.87284, Testing Accuracy 0.8716\n",
      "Epoch 91, Average Loss: 0.005730761559009552, Training Accuracy: 0.87526, Testing Accuracy 0.8673\n",
      "Epoch 92, Average Loss: 0.0056438133461773395, Training Accuracy: 0.87562, Testing Accuracy 0.876\n",
      "Epoch 93, Average Loss: 0.005642904381901025, Training Accuracy: 0.87536, Testing Accuracy 0.8748\n",
      "Epoch 94, Average Loss: 0.005662942672222853, Training Accuracy: 0.87518, Testing Accuracy 0.8857\n",
      "Epoch 95, Average Loss: 0.0056980220115184785, Training Accuracy: 0.87722, Testing Accuracy 0.8758\n",
      "Epoch 96, Average Loss: 0.005682848261296749, Training Accuracy: 0.87676, Testing Accuracy 0.8761\n",
      "Epoch 97, Average Loss: 0.005684170677065849, Training Accuracy: 0.87492, Testing Accuracy 0.8814\n",
      "Epoch 98, Average Loss: 0.00558561893761158, Training Accuracy: 0.87816, Testing Accuracy 0.8768\n",
      "Epoch 99, Average Loss: 0.005640697729736566, Training Accuracy: 0.87702, Testing Accuracy 0.8722\n",
      "Epoch 100, Average Loss: 0.005715971658229828, Training Accuracy: 0.87324, Testing Accuracy 0.8815\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'tab:black' is not a valid value for color",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      2\u001b[0m params \u001b[38;5;241m=\u001b[39m trainingLoop(num_epochs,train_iter,test_iter ,model, loss, optimizer)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mplotTraining\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# displays graph\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[112], line 10\u001b[0m, in \u001b[0;36mplotTraining\u001b[1;34m(num_epoch, loss_values, train_accuracy_values, test_accuracy_values)\u001b[0m\n\u001b[0;32m      8\u001b[0m color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtab:black\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m ax1\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[43max1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_ylabel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrain Loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m ax1\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), loss_values, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-o\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39mcolor)\n\u001b[0;32m     12\u001b[0m ax1\u001b[38;5;241m.\u001b[39mtick_params(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, labelcolor\u001b[38;5;241m=\u001b[39mcolor)\n",
      "File \u001b[1;32mc:\\Users\\ramas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\axes\\_base.py:3734\u001b[0m, in \u001b[0;36m_AxesBase.set_ylabel\u001b[1;34m(self, ylabel, fontdict, labelpad, loc, **kwargs)\u001b[0m\n\u001b[0;32m   3727\u001b[0m     y, ha \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3728\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbottom\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   3729\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   3730\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   3731\u001b[0m     }[loc]\n\u001b[0;32m   3732\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(y\u001b[38;5;241m=\u001b[39my, horizontalalignment\u001b[38;5;241m=\u001b[39mha)\n\u001b[1;32m-> 3734\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myaxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_label_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mylabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfontdict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ramas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\axis.py:1823\u001b[0m, in \u001b[0;36mAxis.set_label_text\u001b[1;34m(self, label, fontdict, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fontdict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mupdate(fontdict)\n\u001b[1;32m-> 1823\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1824\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\n",
      "File \u001b[1;32mc:\\Users\\ramas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\text.py:205\u001b[0m, in \u001b[0;36mText.update\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Update bbox last, as it depends on font properties.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m bbox \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m, sentinel)\n\u001b[1;32m--> 205\u001b[0m ret\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentinel:\n\u001b[0;32m    207\u001b[0m     ret\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bbox(bbox))\n",
      "File \u001b[1;32mc:\\Users\\ramas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\artist.py:1209\u001b[0m, in \u001b[0;36mArtist.update\u001b[1;34m(self, props)\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, props):\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;124;03m    Update this artist's properties from the dict *props*.\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;124;03m    props : dict\u001b[39;00m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_props\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{cls.__name__!r}\u001b[39;49;00m\u001b[38;5;124;43m object has no property \u001b[39;49m\u001b[38;5;132;43;01m{prop_name!r}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ramas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\artist.py:1195\u001b[0m, in \u001b[0;36mArtist._update_props\u001b[1;34m(self, props, errfmt)\u001b[0m\n\u001b[0;32m   1192\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n\u001b[0;32m   1193\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1194\u001b[0m                     errfmt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), prop_name\u001b[38;5;241m=\u001b[39mk))\n\u001b[1;32m-> 1195\u001b[0m             ret\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[0;32m   1197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpchanged()\n",
      "File \u001b[1;32mc:\\Users\\ramas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\text.py:993\u001b[0m, in \u001b[0;36mText.set_color\u001b[1;34m(self, color)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;66;03m# \"auto\" is only supported by axisartist, but we can just let it error\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# out at draw time for simplicity.\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_str_equal(color, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 993\u001b[0m     \u001b[43mmpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_color_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_color \u001b[38;5;241m=\u001b[39m color\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ramas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\colors.py:246\u001b[0m, in \u001b[0;36m_check_color_like\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_color_like(v):\n\u001b[1;32m--> 246\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid value for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: 'tab:black' is not a valid value for color"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "params = trainingLoop(num_epochs,train_iter,test_iter ,model, loss, optimizer)\n",
    "plotTraining(*params) # displays graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
