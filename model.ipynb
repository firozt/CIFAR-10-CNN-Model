{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch import Tensor\n",
    "import torchvision\n",
    "import PIL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting device (checking if gpu is available for faster training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10(batch_size):\n",
    "    # Transformations for the training data\n",
    "    train_trans = transforms.Compose([\n",
    "        # transforms.RandomAffine(degrees=(-5, 5), translate=(0.1, 0.1), scale=(0.9, 1.1), resample=PIL.Image.BILINEAR),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Transformations for the test data (minimal, without augmentation)\n",
    "    test_trans = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Loading the datasets\n",
    "    train = torchvision.datasets.CIFAR10(root=\"../data\", train=True, transform=train_trans, download=True)\n",
    "    test = torchvision.datasets.CIFAR10(root=\"../data\", train=False, transform=test_trans, download=True)\n",
    "    \n",
    "    # Creating the DataLoaders\n",
    "    train_loader = DataLoader(train, batch_size, shuffle=True, pin_memory=False)\n",
    "    test_loader = DataLoader(test, batch_size, shuffle=False, pin_memory=False)\n",
    "    \n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "train_iter, test_iter = load_cifar10(BATCH_SIZE)\n",
    "for data in train_iter:\n",
    "    print(data[0].size())\n",
    "    print(data[1].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 Dataset Analysis\n",
    "Cifar-10 holds images if dimension 32x32\n",
    "Cifar images also contain colour using the rgb format\n",
    "Therefore each item in the dataset is of size 3x32x32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "B1_OUT = 32\n",
    "B2_OUT = 64\n",
    "B3_OUT = 128\n",
    "B4_OUT = B3_OUT\n",
    "L1_OUT = 1024\n",
    "L2_OUT = 512\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.maxpool = nn.MaxPool2d(2,2) \n",
    "        self.maxpool2 = nn.MaxPool2d(2) \n",
    "        self.block1 = Block(6,in_channels,B1_OUT, kernel_size=5, padding=2) \n",
    "        # self.block1 = Block(16,in_channels,B1_OUT, kernel_size=7, padding=3) \n",
    "        self.resid1 = Block(6,B1_OUT,B1_OUT,kernel_size=5,padding=2,residual=True)\n",
    "        self.block2 = Block(6,B1_OUT,B2_OUT, kernel_size=5, padding=2)\n",
    "        self.resid2 = Block(6,B2_OUT,B2_OUT,kernel_size=5,padding=2,residual=True)\n",
    "        self.block3 = Block(6,B2_OUT,B3_OUT,kernel_size=3, padding=1)\n",
    "        self.resid3 = Block(6,B3_OUT,B3_OUT,kernel_size=3,padding=1, residual=True)\n",
    "        # NEW HERE\n",
    "        self.resid3_2 = Block(6,B3_OUT,B3_OUT,kernel_size=3,padding=1, residual=True)\n",
    "        self.block4 = Block(6,B3_OUT,B4_OUT,kernel_size=3,padding=1)\n",
    "        self.resid4 = Block(6,B4_OUT,B4_OUT,kernel_size=3,padding=1, residual=True)\n",
    "        ##\n",
    "        self.spatial_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # Linear Layers\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.activation = nn.Sigmoid()\n",
    "        \n",
    "        self.linear1 = nn.Linear(B4_OUT,L1_OUT)\n",
    "        self.dropout1 = nn.Dropout(0.15)\n",
    "        self.linear2 = nn.Linear(L1_OUT,L2_OUT)\n",
    "        self.dropout2 = nn.Dropout(0.15)\n",
    "        self.linear3 = nn.Linear(L2_OUT, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.block1(x))\n",
    "        # out = self.activation(self.resid1(out))\n",
    "        out = self.maxpool(out) #16x16\n",
    "        out = self.activation(self.block2(out))\n",
    "        out = self.activation(self.resid2(out))\n",
    "        out = self.maxpool(out) # 8x8\n",
    "        out = self.activation(self.block3(out))\n",
    "        out = self.activation(self.resid3(out))\n",
    "        # NEW HERE\n",
    "        out = self.activation(self.resid3_2(out))\n",
    "        out = self.maxpool(out) # 4x4\n",
    "        # out = self.activation(self.block4(out))\n",
    "        ##\n",
    "\n",
    "        out = self.spatial_pool(out)\n",
    "        # flatten for linear layers\n",
    "        out = out.reshape(-1 , B4_OUT)\n",
    "\n",
    "        out = self.linear1(out)\n",
    "        out = self.activation(out)\n",
    "        # out = self.dropout1(out)\n",
    "\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation(out)\n",
    "        # out = self.dropout2(out)\n",
    "\n",
    "        out = self.linear3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, k, in_channels, out_channels, padding=2, kernel_size=5, stride=1, residual=False):\n",
    "        super(Block, self).__init__()\n",
    "        self.residual = residual\n",
    "        L1_OUT = 512\n",
    "        L2_OUT = 512\n",
    "        \n",
    "        self.k = k\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "        # defining weight calculations (a)\n",
    "        self.spatialAvgPool = nn.AdaptiveAvgPool2d(1)\n",
    "        # flatten\n",
    "        self.aLinear1 = nn.Linear(in_features=in_channels, out_features=L1_OUT)\n",
    "        self.aLinear2 = nn.Linear(in_features=L1_OUT,out_features=L2_OUT)\n",
    "        self.aLinear3 = nn.Linear(L2_OUT,k)\n",
    "        self.activation = nn.ReLU()\n",
    "        # defining the convolution section\n",
    "        # creates k convolutions\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size ,padding=padding, stride=stride) for _ in range(k)])\n",
    "        # self.batch_norms = nn.ModuleList([nn.BatchNorm2d(out_channels) for _ in range(k)])\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "        # apply spatial average pooling\n",
    "        pooled: Tensor = self.spatialAvgPool(x)\n",
    "        # flatten the output for the linear layer\n",
    "        pooled = pooled.view(pooled.size(0), -1)\n",
    "        a: Tensor = self.activation(self.aLinear1(pooled))\n",
    "        a = self.activation(self.aLinear2(a))\n",
    "        a = self.activation(self.aLinear3(a))\n",
    "\n",
    "        O: Tensor = torch.zeros_like(self.convs[0](x))\n",
    "\n",
    "        # Apply the convolutions and accumulate the weighted sum\n",
    "        for i in range(self.k):\n",
    "            conv = self.convs[i](x)\n",
    "            conv = self.activation(conv)\n",
    "            # conv = self.batch_norms[i](conv)\n",
    "            O = O + a[:, i].view(-1, 1, 1, 1) * conv # this calculates a_1 conv_1 + ... + a_k conv_k, also applies activation function to it\n",
    "        O = self.batch_norm(O)\n",
    "        \n",
    "        if self.residual:\n",
    "            O= O + x\n",
    "        \n",
    "        return O # returns [batchsize, c, h, w]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    \n",
    "    def __init__(self, n) -> None:\n",
    "        self.data = [0.0] * n\n",
    "    \n",
    "    # deletes all data and information stored\n",
    "    def reset(self) -> None:\n",
    "        self.data = [0.0] * len(self.data)\n",
    "    \n",
    "    # takes in n inputs, for each arg it adds it to its corresponding index in data\n",
    "    def add(self, *args) -> None:\n",
    "        self.data = [a + float(b) for a,b in zip(self.data, args)]\n",
    "    \n",
    "    # allows the indexing operator to be used\n",
    "    def __getitem__(self, idx) -> any:\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def toList(self) -> list:\n",
    "        return self.data\n",
    "    \n",
    "    def percentage(self, index, total):\n",
    "        return 100 * self[index] / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def trainingLoop(num_epoch, train_iter, test_iter, net, loss_function, optimizer) -> tuple[int, list[float], list[float], list[float]]:\n",
    "    loss_values = [] \n",
    "    training_accuracy_values = []  \n",
    "    testing_accuracy_values = []\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        # train on trianing set to obtain updated weights\n",
    "        train_metrics = train(train_iter, net, loss_function, optimizer) # tuple (loss , accuracy)\n",
    "        # testing new weights on unseen data\n",
    "        test_accuracy = test(test_iter, net)\n",
    "        # data for plotting\n",
    "        loss_values.append(train_metrics[0])\n",
    "        training_accuracy_values.append(train_metrics[1]) \n",
    "        testing_accuracy_values.append(test_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {loss_values[-1]}, Training Accuracy: {training_accuracy_values[-1]}, Testing Accuracy {testing_accuracy_values[-1]}')\n",
    "        \n",
    "    return(num_epoch, loss_values, training_accuracy_values, testing_accuracy_values)\n",
    "\n",
    "\n",
    "def test(data_iter, net) -> float:\n",
    "    net.eval() # set to testing mode\n",
    "    \n",
    "    metrics = Accumulator(2) # [correct, total]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            # Move data to the specified device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Get model predictions\n",
    "            y_hat = net(X)\n",
    "            # Convert probabilities to predicted class labels\n",
    "            _, predicted_labels = torch.max(y_hat, 1)\n",
    "            # Accumulate the number of correct predictions and the total\n",
    "            metrics.add((predicted_labels == y).sum().item(), y.size(0))\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = metrics[0] / metrics[1]\n",
    "    \n",
    "    # Set the network back to training mode\n",
    "    net.train()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def train(data_iter, net, loss_function, optimizer) -> tuple[float , float]:\n",
    "    net.train()\n",
    "    \n",
    "    metrics = Accumulator(3)  # [sum of losses, correct predictions, total predictions]\n",
    "    for X, Y in data_iter:  # get x and corresponding y value\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        \n",
    "        y_hat = net(X)  # get prediction\n",
    "        loss = loss_function(y_hat, Y)  # loss of this specific value\n",
    "        optimizer.zero_grad()  # clear gradient\n",
    "        loss.backward()  # calculate derivative of loss w.r.t the weight\n",
    "        optimizer.step()  # change weight values accordingly\n",
    "\n",
    "        metrics.add(loss.item(), (torch.max(y_hat, 1)[1] == Y).float().sum().item(), Y.size(0))\n",
    "    # Calculate average loss and accuracy from the accumulated values\n",
    "    avg_loss = metrics[0] / metrics[2]\n",
    "    accuracy = metrics[1] / metrics[2]\n",
    "    \n",
    "    return (avg_loss, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the data\n",
    "def plotTraining(num_epoch, loss_values, train_accuracy_values, test_accuracy_values):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Training Loss\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Train Loss', color=color)\n",
    "    ax1.plot(range(1, num_epoch+1), loss_values, '-o', color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Create a second y-axis to share the same x-axis\n",
    "    ax2 = ax1.twinx()  \n",
    "\n",
    "    # Training Accuracy and Testing Accuracy\n",
    "    color_train = 'tab:blue'\n",
    "    color_test = 'tab:green'\n",
    "    ax2.set_ylabel('Accuracy', color=color_train)  # We already handled the x-label with ax1\n",
    "    # Plot training accuracy on ax2\n",
    "    ax2.plot(range(1, num_epoch+1), train_accuracy_values, '-s', color=color_train, label='Train Accuracy')\n",
    "    # Plot testing accuracy on ax2 as well\n",
    "    ax2.plot(range(1, num_epoch+1), test_accuracy_values, '-^', color=color_test, label='Test Accuracy')\n",
    "    ax2.tick_params(axis='y', labelcolor=color_train)\n",
    "    ax2.legend(loc='upper left')\n",
    "\n",
    "    fig.tight_layout()  # Otherwise the right y-label is slightly clipped\n",
    "    plt.title('Training Loss and Accuracy')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d: # by checking the type we can init different layers in different ways\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        # torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(3, 10).to(device)\n",
    "# model = Model(3, 10)\n",
    "model.apply(init_weights)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-4 ) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.01532512929201126, Training Accuracy: 0.28592, Testing Accuracy 0.3768\n",
      "Epoch 2, Average Loss: 0.01217972076177597, Training Accuracy: 0.41876, Testing Accuracy 0.4284\n",
      "Epoch 3, Average Loss: 0.01099575785636902, Training Accuracy: 0.48424, Testing Accuracy 0.5011\n",
      "Epoch 4, Average Loss: 0.010141730097532272, Training Accuracy: 0.52794, Testing Accuracy 0.5357\n",
      "Epoch 5, Average Loss: 0.009533973472118379, Training Accuracy: 0.55994, Testing Accuracy 0.5707\n",
      "Epoch 6, Average Loss: 0.008932209870815278, Training Accuracy: 0.58984, Testing Accuracy 0.5705\n",
      "Epoch 7, Average Loss: 0.008575738742351532, Training Accuracy: 0.60816, Testing Accuracy 0.6162\n",
      "Epoch 8, Average Loss: 0.008249762881994247, Training Accuracy: 0.61988, Testing Accuracy 0.6471\n",
      "Epoch 9, Average Loss: 0.007975986238718032, Training Accuracy: 0.63702, Testing Accuracy 0.602\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[312], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# mu.train_ch3(model, train_iter, test_iter, loss, num_epochs, optimizer)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mtrainingLoop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m plotTraining(\u001b[38;5;241m*\u001b[39mparams) \u001b[38;5;66;03m# spreads tuple\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mincrease mlp layer width -> double\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mincrease channels\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[308], line 10\u001b[0m, in \u001b[0;36mtrainingLoop\u001b[1;34m(num_epoch, train_iter, test_iter, net, loss_function, optimizer)\u001b[0m\n\u001b[0;32m      6\u001b[0m testing_accuracy_values \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epoch):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# train on trianing set to obtain updated weights\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# tuple (loss , accuracy)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# testing new weights on unseen data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     test_accuracy \u001b[38;5;241m=\u001b[39m test(test_iter, net)\n",
      "Cell \u001b[1;32mIn[308], line 57\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data_iter, net, loss_function, optimizer)\u001b[0m\n\u001b[0;32m     55\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(y_hat, Y)  \u001b[38;5;66;03m# loss of this specific value\u001b[39;00m\n\u001b[0;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# clear gradient\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# calculate derivative of loss w.r.t the weight\u001b[39;00m\n\u001b[0;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# change weight values accordingly\u001b[39;00m\n\u001b[0;32m     60\u001b[0m metrics\u001b[38;5;241m.\u001b[39madd(loss\u001b[38;5;241m.\u001b[39mitem(), (torch\u001b[38;5;241m.\u001b[39mmax(y_hat, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m Y)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem(), Y\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\ramas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ramas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "# mu.train_ch3(model, train_iter, test_iter, loss, num_epochs, optimizer)\n",
    "params = trainingLoop(num_epochs,train_iter,test_iter ,model, loss, optimizer)\n",
    "plotTraining(*params) # spreads tuple\n",
    "\n",
    "\"\"\"\n",
    "increase mlp layer width -> double\n",
    "increase channels\n",
    "increase k\n",
    "\n",
    "\n",
    "=======\n",
    "\n",
    "sgd\n",
    "\n",
    "1 max 2 max 3 (2 resids)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
