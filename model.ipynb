{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch import Tensor\n",
    "import torchvision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting device (checking if gpu is available for faster training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_cifar10(batch_size):\n",
    "    trans = [\n",
    "        transforms.ToTensor(),transforms.RandomRotation(10), transforms.RandomHorizontalFlip()]\n",
    "    trans = transforms.Compose(trans)\n",
    "    train = torchvision.datasets.CIFAR10(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    test = torchvision.datasets.CIFAR10(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (DataLoader(train, batch_size, shuffle=True, pin_memory=False),\n",
    "            DataLoader(test, batch_size, shuffle=False, pin_memory=False))\n",
    "\n",
    "def load_cifar10(batch_size):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    trans = transforms.Compose(trans)\n",
    "    train = torchvision.datasets.CIFAR10(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    test = torchvision.datasets.CIFAR10(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (DataLoader(train, batch_size, shuffle=True, pin_memory=False),\n",
    "            DataLoader(test, batch_size, shuffle=False, pin_memory=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_iter, test_iter = load_cifar10(BATCH_SIZE)\n",
    "for data in train_iter:\n",
    "    print(data[0].size())\n",
    "    print(data[1].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 Dataset Analysis\n",
    "Cifar-10 holds images if dimension 32x32\n",
    "Cifar images also contain colour using the rgb format\n",
    "Therefore each item in the dataset is of size 3x32x32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hyper params\n",
    "B1_OUT = 16\n",
    "B2_OUT = 32\n",
    "B3_OUT = 16\n",
    "L1_OUT = 4000\n",
    "L2_OUT = 500\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_features):\n",
    "        super(Model, self).__init__()\n",
    "        # 2 Blocks then 3 linear layers\n",
    "\n",
    "        \n",
    "        self.block1 = Block(16,in_channels,B1_OUT,kernel_size=3,padding=1) # 32x32\n",
    "        self.block2 = Block(64,B1_OUT,B2_OUT, kernel_size=4,stride=2) #17x17\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # 17x17\n",
    "        \n",
    "        # Linear Layers\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(B2_OUT*17*17,L1_OUT)\n",
    "        self.linear2 = nn.Linear(L1_OUT,L2_OUT)\n",
    "        self.linear3 = nn.Linear(L2_OUT, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batchsize, in_channels, 32, 32]\n",
    "        out: Tensor = self.block1(x)\n",
    "        # [batchsize, B1_OUT, 32, 32]\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        # [batchsize, B1_OUT, 32, 32]\n",
    "        out = self.block2(out)\n",
    "        # [batchsize, B2_OUT, 17, 17]\n",
    "        out = self.activation(out)\n",
    "\n",
    "        # flatten for linear layers\n",
    "        # print(out.size())\n",
    "        out = out.reshape(-1 , B2_OUT*17*17)\n",
    "        out = self.linear1(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        out = self.linear2(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        out = self.linear3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, k, in_channels, out_channels, padding=2, kernel_size=5, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.k = k\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "        # defining weight calculations (a)\n",
    "        self.spatialAvgPool = nn.AdaptiveAvgPool2d(1)\n",
    "        # flatten\n",
    "        self.aLinear1 = nn.Linear(in_features=in_channels, out_features=k) \n",
    "        self.activation = nn.ReLU()\n",
    "        # defining the convolution section\n",
    "        # creates k convolutions\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size ,padding=padding, stride=stride) for _ in range(k)])\n",
    "        \n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \n",
    "        # apply spatial average pooling\n",
    "        pooled: Tensor = self.spatialAvgPool(x)\n",
    "        # flatten the output for the linear layer\n",
    "        pooled = pooled.view(pooled.size(0), -1)\n",
    "        a: Tensor = self.activation(self.aLinear1(pooled)) # creates vector of size k\n",
    "        \n",
    "        O: Tensor = torch.zeros_like(self.convs[0](x))\n",
    "        \n",
    "        # Apply the convolutions and accumulate the weighted sum\n",
    "        for i in range(self.k):\n",
    "            O = O + self.activation(a[:, i].view(-1, 1, 1, 1) * self.convs[i](x)) # this calculates a_1 conv_1 + ... + a_k conv_k, also applies activation function to it\n",
    "        \n",
    "        return O # returns [batchsize, c, h, w]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    \n",
    "    def __init__(self, n) -> None:\n",
    "        self.data = [0.0] * n\n",
    "    \n",
    "    # deletes all data and information stored\n",
    "    def reset(self) -> None:\n",
    "        self.data = [0.0] * len(self.data)\n",
    "    \n",
    "    # takes in n inputs, for each arg it adds it to its corresponding index in data\n",
    "    def add(self, *args) -> None:\n",
    "        self.data = [a + float(b) for a,b in zip(self.data, args)]\n",
    "    \n",
    "    # allows the indexing operator to be used\n",
    "    def __getitem__(self, idx) -> any:\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def toList(self) -> list:\n",
    "        return self.data\n",
    "    \n",
    "    def percentage(self, index, total):\n",
    "        return 100 * self[index] / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def trainingLoop(num_epoch, train_iter, test_iter, net, loss_function, optimizer) -> tuple[int, list[float], list[float], list[float]]:\n",
    "    loss_values = [] \n",
    "    training_accuracy_values = []  \n",
    "    testing_accuracy_values = []\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        # train on trianing set to obtain updated weights\n",
    "        train_metrics = train(train_iter, net, loss_function, optimizer) # tuple (loss , accuracy)\n",
    "        # testing new weights on unseen data\n",
    "        test_accuracy = test(test_iter, net)\n",
    "        # data for plotting\n",
    "        loss_values.append(train_metrics[0])\n",
    "        training_accuracy_values.append(train_metrics[1]) \n",
    "        testing_accuracy_values.append(test_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {loss_values[-1]}, Training Accuracy: {training_accuracy_values[-1]}, Testing Accuracy {testing_accuracy_values[-1]}')\n",
    "        \n",
    "    return(num_epoch, loss_values, training_accuracy_values, testing_accuracy_values)\n",
    "\n",
    "\n",
    "def test(data_iter, net) -> float:\n",
    "    net.eval() # set to testing mode\n",
    "    \n",
    "    metrics = Accumulator(2) # [correct, total]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            # Move data to the specified device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Get model predictions\n",
    "            y_hat = net(X)\n",
    "            # Convert probabilities to predicted class labels\n",
    "            _, predicted_labels = torch.max(y_hat, 1)\n",
    "            # Accumulate the number of correct predictions and the total\n",
    "            metrics.add((predicted_labels == y).sum().item(), y.size(0))\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = metrics[0] / metrics[1]\n",
    "    \n",
    "    # Set the network back to training mode\n",
    "    net.train()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def train(data_iter, net, loss_function, optimizer) -> tuple[float , float]:\n",
    "    net.train()\n",
    "    \n",
    "    metrics = Accumulator(3)  # [sum of losses, correct predictions, total predictions]\n",
    "    for X, Y in data_iter:  # get x and corresponding y value\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        \n",
    "        y_hat = net(X)  # get prediction\n",
    "        loss = loss_function(y_hat, Y)  # loss of this specific value\n",
    "        optimizer.zero_grad()  # clear gradient\n",
    "        loss.backward()  # calculate derivative of loss w.r.t the weight\n",
    "        optimizer.step()  # change weight values accordingly\n",
    "\n",
    "        metrics.add(loss.item(), (torch.max(y_hat, 1)[1] == Y).float().sum().item(), Y.size(0))\n",
    "    # Calculate average loss and accuracy from the accumulated values\n",
    "    avg_loss = metrics[0] / metrics[2]\n",
    "    accuracy = metrics[1] / metrics[2]\n",
    "    \n",
    "    return (avg_loss, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the data\n",
    "def plotTraining(num_epoch, loss_values, train_accuracy_values, test_accuracy_values):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Training Loss\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Train Loss', color=color)\n",
    "    ax1.plot(range(1, num_epoch+1), loss_values, '-o', color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Create a second y-axis to share the same x-axis\n",
    "    ax2 = ax1.twinx()  \n",
    "\n",
    "    # Training Accuracy and Testing Accuracy\n",
    "    color_train = 'tab:blue'\n",
    "    color_test = 'tab:green'\n",
    "    ax2.set_ylabel('Accuracy', color=color_train)  # We already handled the x-label with ax1\n",
    "    # Plot training accuracy on ax2\n",
    "    ax2.plot(range(1, num_epoch+1), train_accuracy_values, '-s', color=color_train, label='Train Accuracy')\n",
    "    # Plot testing accuracy on ax2 as well\n",
    "    ax2.plot(range(1, num_epoch+1), test_accuracy_values, '-^', color=color_test, label='Test Accuracy')\n",
    "    ax2.tick_params(axis='y', labelcolor=color_train)\n",
    "    ax2.legend(loc='upper left')\n",
    "\n",
    "    fig.tight_layout()  # Otherwise the right y-label is slightly clipped\n",
    "    plt.title('Training Loss and Accuracy')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d: # by checking the type we can init different layers in different ways\n",
    "        # torch.nn.init.xavier_normal_(m.weight)\n",
    "        torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(3, 10).to(device)\n",
    "# model = Model(3, 10)\n",
    "model.apply(init_weights)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9,0.999),eps=1e-8 ) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.036104674243927, Training Accuracy: 0.09996, Testing Accuracy 0.1296\n",
      "Epoch 2, Average Loss: 0.03059112731933594, Training Accuracy: 0.28234, Testing Accuracy 0.4198\n",
      "Epoch 3, Average Loss: 0.022866991243362427, Training Accuracy: 0.47312, Testing Accuracy 0.4987\n",
      "Epoch 4, Average Loss: 0.019513206915855407, Training Accuracy: 0.55464, Testing Accuracy 0.5787\n",
      "Epoch 5, Average Loss: 0.01645569652915001, Training Accuracy: 0.62708, Testing Accuracy 0.6021\n",
      "Epoch 6, Average Loss: 0.012936260107159614, Training Accuracy: 0.70612, Testing Accuracy 0.6004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "# mu.train_ch3(model, train_iter, test_iter, loss, num_epochs, optimizer)\n",
    "params = trainingLoop(num_epochs,train_iter,test_iter ,model, loss, optimizer)\n",
    "plotTraining(*params) # spreads tuple"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
